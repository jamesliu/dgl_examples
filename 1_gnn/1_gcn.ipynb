{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\nGraph Convolutional Network\n====================================\n\n**Author:** `Qi Huang <https://github.com/HQ01>`_, `Minjie Wang  <https://jermainewang.github.io/>`_,\nYu Gai, Quan Gan, Zheng Zhang\n\nThis is a gentle introduction of using DGL to implement Graph Convolutional\nNetworks (Kipf & Welling et al., `Semi-Supervised Classificaton with Graph\nConvolutional Networks <https://arxiv.org/pdf/1609.02907.pdf>`_). We build upon\nthe :doc:`earlier tutorial <../../basics/3_pagerank>` on DGLGraph and demonstrate\nhow DGL combines graph with deep neural network and learn structural representations.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model Overview\n------------------------------------------\nGCN from the perspective of message passing\n```````````````````````````````````````````````\nWe describe a layer of graph convolutional neural network from a message\npassing perspective; the math can be found `here <math_>`_.\nIt boils down to the following step, for each node $u$:\n\n1) Aggregate neighbors' representations $h_{v}$ to produce an\nintermediate representation $\\hat{h}_u$.  2) Transform the aggregated\nrepresentation $\\hat{h}_{u}$ with a linear projection followed by a\nnon-linearity: $h_{u} = f(W_{u} \\hat{h}_u)$.\n\nWe will implement step 1 with DGL message passing, and step 2 with the\n``apply_nodes`` method, whose node UDF will be a PyTorch ``nn.Module``.\n\nGCN implementation with DGL\n``````````````````````````````````````````\nWe first define the message and reduce function as usual.  Since the\naggregation on a node $u$ only involves summing over the neighbors'\nrepresentations $h_v$, we can simply use builtin functions:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import dgl\nimport dgl.function as fn\nimport torch as th\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom dgl import DGLGraph\n\ngcn_msg = fn.copy_src(src='h', out='m')\ngcn_reduce = fn.sum(msg='m', out='h')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then define the node UDF for ``apply_nodes``, which is a fully-connected layer:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class NodeApplyModule(nn.Module):\n    def __init__(self, in_feats, out_feats, activation):\n        super(NodeApplyModule, self).__init__()\n        self.linear = nn.Linear(in_feats, out_feats)\n        self.activation = activation\n\n    def forward(self, node):\n        h = self.linear(node.data['h'])\n        h = self.activation(h)\n        return {'h' : h}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then proceed to define the GCN module. A GCN layer essentially performs\nmessage passing on all the nodes then applies the `NodeApplyModule`. Note\nthat we omitted the dropout in the paper for simplicity.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class GCN(nn.Module):\n    def __init__(self, in_feats, out_feats, activation):\n        super(GCN, self).__init__()\n        self.apply_mod = NodeApplyModule(in_feats, out_feats, activation)\n\n    def forward(self, g, feature):\n        g.ndata['h'] = feature\n        g.update_all(gcn_msg, gcn_reduce)\n        g.apply_nodes(func=self.apply_mod)\n        return g.ndata.pop('h')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The forward function is essentially the same as any other commonly seen NNs\nmodel in PyTorch.  We can initialize GCN like any ``nn.Module``. For example,\nlet's define a simple neural network consisting of two GCN layers. Suppose we\nare training the classifier for the cora dataset (the input feature size is\n1433 and the number of classes is 7).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.gcn1 = GCN(1433, 16, F.relu)\n        self.gcn2 = GCN(16, 7, F.relu)\n    \n    def forward(self, g, features):\n        x = self.gcn1(g, features)\n        x = self.gcn2(g, x)\n        return x\nnet = Net()\nprint(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We load the cora dataset using DGL's built-in data module.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from dgl.data import citation_graph as citegrh\ndef load_cora_data():\n    data = citegrh.load_cora()\n    features = th.FloatTensor(data.features)\n    labels = th.LongTensor(data.labels)\n    mask = th.ByteTensor(data.train_mask)\n    g = DGLGraph(data.graph)\n    return g, features, labels, mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then train the network as follows:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import time\nimport numpy as np\ng, features, labels, mask = load_cora_data()\noptimizer = th.optim.Adam(net.parameters(), lr=1e-3)\ndur = []\nfor epoch in range(30):\n    if epoch >=3:\n        t0 = time.time()\n        \n    logits = net(g, features)\n    logp = F.log_softmax(logits, 1)\n    loss = F.nll_loss(logp[mask], labels[mask])\n    \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    if epoch >=3:\n        dur.append(time.time() - t0)\n    \n    print(\"Epoch {:05d} | Loss {:.4f} | Time(s) {:.4f}\".format(\n            epoch, loss.item(), np.mean(dur)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nGCN in one formula\n------------------\nMathematically, the GCN model follows this formula:\n\n$H^{(l+1)} = \\sigma(\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}H^{(l)}W^{(l)})$\n\nHere, $H^{(l)}$ denotes the $l^{th}$ layer in the network,\n$\\sigma$ is the non-linearity, and $W$ is the weight matrix for\nthis layer. $D$ and $A$, as commonly seen, represent degree\nmatrix and adjacency matrix, respectively. The ~ is a renormalization trick\nin which we add a self-connection to each node of the graph, and build the\ncorresponding degree and adjacency matrix.  The shape of the input\n$H^{(0)}$ is $N \\times D$, where $N$ is the number of nodes\nand $D$ is the number of input features. We can chain up multiple\nlayers as such to produce a node-level representation output with shape\n:math`N \\times F`, where $F$ is the dimension of the output node\nfeature vector.\n\nThe equation can be efficiently implemented using sparse matrix\nmultiplication kernels (such as Kipf's\n`pygcn <https://github.com/tkipf/pygcn>`_ code). The above DGL implementation\nin fact has already used this trick due to the use of builtin functions. To\nunderstand what is under the hood, please read our tutorial on :doc:`PageRank <../../basics/3_pagerank>`.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}