{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\nStochastic Steady-state Embedding (SSE)\n=======================================\n\n**Author**: Gai Yu, Da Zheng, Quan Gan, Jinjing Zhou, Zheng Zhang\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\\begin{align}\\newcommand{\\bfy}{\\textbf{y}}\n   \\newcommand{\\cale}{{\\mathcal{E}}}\n   \\newcommand{\\calg}{{\\mathcal{G}}}\n   \\newcommand{\\call}{{\\mathcal{L}}}\n   \\newcommand{\\caln}{{\\mathcal{N}}}\n   \\newcommand{\\calo}{{\\mathcal{O}}}\n   \\newcommand{\\calt}{{\\mathcal{T}}}\n   \\newcommand{\\calv}{{\\mathcal{V}}}\n   \\newcommand{\\until}{\\text{until}\\ }\\end{align}\n\nIn this tutorial we implement in DGL with MXNet\n\n-  Simple steady-state algorithms with `stochastic steady-state\n   embedding <https://www.cc.gatech.edu/~hdai8/pdf/equilibrium_embedding.pdf>`__\n   (SSE), and\n-  Training with subgraph sampling.\n\nSubgraph sampling is a generic technique to scale up learning to\ngigantic graphs (e.g. with billions of nodes and edges). It can apply to\nother algorithms, such as :doc:`Graph convolution\nnetwork <1_gcn>`\nand :doc:`Relational graph convolution\nnetwork <4_rgcn>`.\n\nSteady-state algorithms\n-----------------------\n\nMany algorithms for graph analytics are iterative procedures that\nterminate when some steady states are reached. Examples include\nPageRank, and mean-field inference on Markov Random Fields.\n\nFlood-fill algorithm\n~~~~~~~~~~~~~~~~~~~~\n\n*Flood-fill algorithm* (or *infection* algorithm as in Dai et al.) can\nalso be seen as such a procedure. Specifically, the problem is that\ngiven a graph $\\calg = (\\calv, \\cale)$ and a source node\n$s \\in \\calv$, we need to mark all nodes that can be reached from\n$s$. Let $\\calv = \\{1, ..., n\\}$ and let $y_v$\nindicate whether a node $v$ is marked. The flood-fill algorithm\nproceeds as follows:\n\n\\begin{align}\\begin{alignat}{2}\n   & y_s^{(0)} \\leftarrow 1 \\tag{0} \\\\\n   & y_v^{(0)} \\leftarrow 0 \\qquad && v \\ne s \\tag{1} \\\\\n   & y_v^{(t + 1)} \\leftarrow \\max_{u \\in \\caln (v)} y_u^{(t)} \\qquad && \\until \\forall v \\in \\calv, y_v^{(t + 1)} = y_v^{(t)} \\tag{2}\n   \\end{alignat}\\end{align}\n\n\nwhere $\\caln (v)$ denotes the neighborhood of $v$, including\n$v$ itself.\n\nThe flood-fill algorithm first marks the source node $s$, and then\nrepeatedly marks nodes with one or more marked neighbors until no node\nneeds to be marked, i.e. the steady state is reached.\n\nFlood-fill algorithm and steady-state operator\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nMore abstractly, $\\begin{align}\n& y_v^{(0)} \\leftarrow \\text{constant} \\\\\n& \\bfy^{(t + 1)} \\leftarrow \\calt (\\bfy^{(t)}) \\qquad \\until \\bfy^{(t + 1)} = \\bfy^{(t)} \\tag{3}\n\\end{align}$ where $\\bfy^{(t)} = (y_1^{(t)}, ..., y_n^{(t)})$ and\n$[\\calt (\\bfy^{(t)})]_v = \\hat\\calt (\\{\\bfy_u^{(t)} : u \\in \\caln (v)\\})$.\nIn the case of the flood-fill algorithm, $\\hat\\calt = \\max$. The\ncondition \u201c$\\until \\bfy^{(t + 1)} = \\bfy^{(t)}$\u201d in $(3)$\nimplies that $\\bfy^*$ is the solution to the problem if and only\nif $\\bfy^* = \\calt (\\bfy^*)$, i.e. \\ $\\bfy^*$ is steady\nunder $\\calt$. Thus we call $\\calt$ the *steady-state\noperator*.\n\nImplementation\n~~~~~~~~~~~~~~\n\nWe can easily implement flood-fill in DGL:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import mxnet as mx\nimport os\nimport dgl\n\ndef T(g):\n    def message_func(edges):\n        return {'m': edges.src['y']}\n    def reduce_func(nodes):\n        # First compute the maximum of all neighbors...\n        m = mx.nd.max(nodes.mailbox['m'], axis=1)\n        # Then compare the maximum with the node itself.\n        # One can also add a self-loop to each node to avoid this\n        # additional max computation.\n        m = mx.nd.maximum(m, nodes.data['y'])\n        return {'y': m.reshape(m.shape[0], 1)}\n    g.update_all(message_func, reduce_func)\n    return g.ndata['y']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To run the algorithm, let\u2019s create a ``DGLGraph`` consisting of two\ndisjoint chains, each with 10 nodes, and initialize it as specified in\nEq. $(0)$ and Eq. $(1)$.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n\ndef disjoint_chains(n_chains, length):\n    path_graph = nx.path_graph(n_chains * length).to_directed()\n    for i in range(n_chains - 1):  # break the path graph into N chains\n        path_graph.remove_edge((i + 1) * length - 1, (i + 1) * length)\n        path_graph.remove_edge((i + 1) * length, (i + 1) * length - 1)\n    for n in path_graph.nodes:\n        path_graph.add_edge(n, n)  # add self connections\n    return path_graph\n\nN = 2    # the number of chains\nL = 500 # the length of a chain\ns = 0    # the source node\n# The sampler (see the subgraph sampling section) only supports\n# readonly graphs.\ng = dgl.DGLGraph(disjoint_chains(N, L), readonly=True)\ny = mx.nd.zeros([g.number_of_nodes(), 1])\ny[s] = 1\ng.ndata['y'] = y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let\u2019s apply ``T`` to ``g`` until convergence. You can see that nodes\nreachable from ``s`` are gradually \u201cinfected\u201d (marked).\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "while True:\n    prev_y = g.ndata['y']\n    next_y = T(g)\n    if all(prev_y == next_y):\n        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The update procedure is visualized as follows:\n\n|image0|\n\nSteady-state embedding\n----------------------\n\nNeural flood-fill algorithm\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nNow let\u2019s consider designing a neural network that simulates the\nflood-fill algorithm.\n\n-  Instead of using $\\calt$ to update the states of nodes, we use\n   $\\calt_\\Theta$, a graph neural network (and\n   $\\hat\\calt_\\Theta$ instead of $\\hat\\calt$).\n-  The state of a node $v$ is no longer a boolean value\n   ($y_v$), but, an embedding $h_v$ (a vector of some\n   reasonable dimension, say, $H$).\n-  We also associate a feature vector $x_v$ with $v$. For\n   the flood-fill algorithm, we simply use the one-hot encoding of a\n   node\u2019s ID as its feature vector, so that our algorithm can\n   distinguish different nodes.\n-  We only iterate $T$ times instead of iterating until the\n   steady-state condition is satisfied.\n-  After iteration, we mark the nodes by passing the node embedding\n   $h_v$ into another neural network to produce a probability\n   $p_v$ of whether the node is reachable.\n\nMathematically, $\\begin{align}\n& h_v^{(0)} \\leftarrow \\text{random embedding} \\\\\n& h_v^{(t + 1)} \\leftarrow \\calt_\\Theta (h_1^{(t)}, ..., h_n^{(t)}) \\qquad 1 \\leq t \\leq T \\tag{4}\n\\end{align}$ where\n$[\\calt_\\Theta (h_1^{(t)}, ..., h_n^{(t)})]_v = \\hat\\calt_\\Theta (x_u, h_u^{(t)} : u \\in \\caln (v)\\})$.\n$\\hat\\calt_\\Theta$ is a two layer neural network, as follows:\n\n\\begin{align}\\hat\\calt_\\Theta (\\{x_u, h_u^{(t)} : u \\in \\caln (v)\\})\n   = W_1 \\sigma \\left(W_2 \\left[x_v, \\sum_{u \\in \\caln (v)} \\left[h_v, x_v\\right]\\right]\\right)\\end{align}\n\nwhere $[\\cdot, \\cdot]$ denotes the concatenation of vectors, and\n$\\sigma$ is a nonlinearity, e.g. ReLU. Essentially, for every\nnode, $\\calt_\\Theta$ repeatedly gathers its neighbors\u2019 feature\nvectors and embeddings, sums them up, and feeds the result along with\nthe node\u2019s own feature vector to a two layer neural network.\n\nImplementation of neural flood-fill\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nLike the naive algorithm, the neural flood-fill algorithm can be\npartitioned into a ``message_func`` (neighborhood information gathering)\nand a ``reduce_func`` ($\\hat\\calt_\\Theta$). We define\n$\\hat\\calt_\\Theta$ as a callable ``gluon.Block``:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import mxnet.gluon as gluon\n\nclass SteadyStateOperator(gluon.Block):\n    def __init__(self, n_hidden, activation, **kwargs):\n        super(SteadyStateOperator, self).__init__(**kwargs)\n        with self.name_scope():\n            self.dense1 = gluon.nn.Dense(n_hidden, activation=activation)\n            self.dense2 = gluon.nn.Dense(n_hidden)\n \n    def forward(self, g):\n        def message_func(edges):\n            x = edges.src['x']\n            h = edges.src['h']\n            return {'m' : mx.nd.concat(x, h, dim=1)}\n \n        def reduce_func(nodes):\n            m = mx.nd.sum(nodes.mailbox['m'], axis=1)\n            z = mx.nd.concat(nodes.data['x'], m, dim=1)\n            return {'h' : self.dense2(self.dense1(z))}\n \n        g.update_all(message_func, reduce_func)\n        return g.ndata['h']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In practice, Eq. $(4)$ may cause numerical instability. One\nsolution is to update $h_v$ with a moving average, as follows:\n\n\\begin{align}h_v^{(t + 1)} \\leftarrow (1 - \\alpha) h_v^{(t)} + \\alpha \\left[\\calt_\\Theta (h_0^{(t)}, ..., h_n^{(t)})\\right]_v \\qquad 0 < \\alpha < 1\\end{align}\n\nPutting these together we have:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def update_embeddings(g, steady_state_operator):\n    prev_h = g.ndata['h']\n    next_h = steady_state_operator(g)\n    g.ndata['h'] = (1 - alpha) * prev_h + alpha * next_h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The last step involves implementing the predictor:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Predictor(gluon.Block):\n    def __init__(self, n_hidden, activation, **kwargs):\n        super(Predictor, self).__init__(**kwargs)\n        with self.name_scope():\n            self.dense1 = gluon.nn.Dense(n_hidden, activation=activation)\n            self.dense2 = gluon.nn.Dense(2)  ## binary classifier\n \n    def forward(self, g):\n        g.ndata['z'] = self.dense2(self.dense1(g.ndata['h']))\n        return g.ndata['z']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The predictor\u2019s decision rule is just a decision rule for binary\nclassification:\n\n\\begin{align}\\hat{y}_v = \\text{argmax}_{i \\in \\{0, 1\\}} \\left[g_\\Phi (h_v^{(T)})\\right]_i \\tag{5}\\end{align}\n\nwhere the predictor is denoted by $g_\\Phi$ and $\\hat{y}_v$\nindicates whether the node $v$ is marked or not.\n\nOur implementation can be further accelerated using DGL's :mod:`built-in\nfunctions <dgl.function>`, which maps\nthe computation to more efficient sparse operators in the backend\nframework (e.g., MXNet/Gluon, PyTorch). Please see\nthe :doc:`Graph convolution network <1_gcn>` tutorial\nfor more details.\n\nEfficient semi-supervised learning on graph\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIn our setting, we can observe the entire structure of one fixed graph as well\nas the feature vector of each node. However, we only have access to the\nlabels of some (very few) of the nodes. We will train the neural\nflood-fill algorithm in this setting as well.\n\nWe initialize feature vectors ``'x'`` and node embeddings ``'h'``\nfirst.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport numpy.random as npr\n\nn = g.number_of_nodes()\nn_hidden = 16\n\ng.ndata['x'] = mx.nd.eye(n, n)\ng.ndata['y'] = mx.nd.concat(*[i * mx.nd.ones([L, 1], dtype='float32')\n                             for i in range(N)], dim=0)\ng.ndata['h'] = mx.nd.zeros([n, n_hidden])\n\nr_train = 0.2  # the ratio of test nodes\nn_train = int(r_train * n)\nnodes_train = npr.choice(range(n), n_train, replace=True)\ntest_bitmap = np.ones(shape=(n))\ntest_bitmap[nodes_train] = 0\nnodes_test = np.where(test_bitmap)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unrolling the iterations in Eq. $(4)$, we have the following\nexpression for updated node embeddings:\n\n\\begin{align}h_v^{(T)} = \\calt_\\Theta^T (h_1^{(0)}, ..., h_n^{(0)}) \\qquad v \\in \\calv \\tag{6}\\end{align}\n\nwhere $\\calt_\\Theta^T$ means applying $\\calt_\\Theta$ for\n$T$ times. These updated node embeddings are fed to $g_\\Phi$\nas in Eq. $(5)$. These steps are fully differentiable and the\nneural flood-fill algorithm can thus be trained in an end-to-end\nfashion. Denoting the binary cross-entropy loss by $l$, we have a\nloss function in the following form:\n\n\\begin{align}\\call (\\Theta, \\Phi) = \\frac1{\\left|\\calv_y\\right|} \\sum_{v \\in \\calv_y} l \\left(g_\\Phi \\left(\\left[\\calt_\\Theta^T (h_1^{(0)}, ..., h_n^{(0)})\\right]_v \\right), y_v\\right) \\tag{7}\\end{align}\n\nAfter computing $\\call (\\Theta, \\Phi)$, we can update\n$\\Theta$ and $\\Phi$ using the gradients\n$\\nabla_\\Theta \\call (\\Theta, \\Phi)$ and\n$\\nabla_\\Phi \\call (\\Theta, \\Phi)$. One problem with Eq.\n$(7)$ is that computing $\\nabla_\\Theta \\call (\\Theta, \\Phi)$\nand $\\nabla_\\Phi \\call (\\Theta, \\Phi)$ requires back-propagating\n$T$ times through $\\calt_\\Theta$, which may be slow in\npractice. So we adopt the following \u201csteady-state\u201d loss function, which\nonly incorporates the last node embedding update in back-propagation:\n\n\\begin{align}\\call_\\text{SteadyState} (\\Theta, \\Phi) = \\frac1{\\left|\\calv_y\\right|} \\sum_{v \\in \\calv_y} l \\left(g_\\Phi \\left(\\left[\\calt_\\Theta (h_1^{(T - 1)}, ..., h_n^{(T - 1)})\\right]_v, y_v\\right)\\right) \\tag{8}\\end{align}\n\nThe following implements one step of training with\n$\\call_\\text{SteadyState}$. Note that ``g`` in the following is\n$\\calg_y$ instead of $\\calg$.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def update_parameters(g, label_nodes, steady_state_operator, predictor, trainer):\n    n = g.number_of_nodes()\n    with mx.autograd.record():\n        steady_state_operator(g)\n        z = predictor(g)[label_nodes]\n        y = g.ndata['y'].reshape(n)[label_nodes]  # label\n        loss = mx.nd.softmax_cross_entropy(z, y)\n    loss.backward()\n    trainer.step(n)  # divide gradients by the number of labelled nodes\n    return loss.asnumpy()[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are now ready to implement the training procedure, which is in two\nphases:\n\n-  The first phase updates node embeddings several times using\n   $\\calt_\\Theta$ to attain an approximately steady state\n-  The second phase trains $\\calt_\\Theta$ and $g_\\Phi$ using\n   this steady state.\n\nNote that we update the node embeddings of $\\calg$ instead of\n$\\calg_y$ only. The reason lies in the semi-supervised learning\nsetting: to do inference on $\\calg$, we need node embeddings on\n$\\calg$ instead of on $\\calg_y$ only.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def train(g, label_nodes, steady_state_operator, predictor, trainer):\n     # first phase\n    for i in range(n_embedding_updates):\n        update_embeddings(g, steady_state_operator)\n    # second phase\n    for i in range(n_parameter_updates):\n        loss = update_parameters(g, label_nodes, steady_state_operator, predictor, trainer)\n    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Scaling up with Stochastic Subgraph Training\n--------------------------------------------\n\nThe computation time per update is linear to the number of edges in a\ngraph. If we have a gigantic graph with billions of nodes and edges, the\nupdate function would be inefficient.\n\nA possible improvement draws analogy from minibatch training on large\ndatasets: instead of computing gradients on the entire graph, we only\nconsider some subgraphs randomly sampled from the labelled nodes.\nMathematically, we have the following loss function:\n\n\\begin{align}\\call_\\text{StochasticSteadyState} (\\Theta, \\Phi) = \\frac1{\\left|\\calv_y^{(k)}\\right|} \\sum_{v \\in \\calv_y^{(k)}} l \\left(g_\\Phi \\left(\\left[\\calt_\\Theta (h_1, ..., h_n)\\right]_v\\right), y_v\\right)\\end{align}\n\nwhere $\\calv_y^{(k)}$ is the subset sampled for iteration\n$k$.\n\nIn this training procedure, we also update node embeddings only on\nsampled subgraphs, which is perhaps not surprising if you know\nstochastic fixed-point iteration.\n\nNeighbor sampling\n~~~~~~~~~~~~~~~~~\n\nWe use *neighbor sampling* as our subgraph sampling strategy. Neighbor\nsampling traverses small neighborhoods from seed nodes with BFS. For\neach newly sampled node, a small subset of neighboring nodes are sampled\nand added to the subgraph along with the connecting edges, unless the\nnode reaches the maximum of $k$ hops from the seeding node.\n\nThe following shows neighbor sampling with 2 seed nodes at a time, a\nmaximum of 2 hops, and a maximum of 3 neighboring nodes.\n\n|image1|\n\nDGL supports very efficient subgraph sampling natively to help users\nscale algorithms to large graphs. Currently, DGL provides the\n:func:`~dgl.contrib.sampling.sampler.NeighborSampler`\nAPI, which returns a subgraph iterator that samples multiple subgraphs\nat a time with neighbor sampling.\n\nThe following code demonstrates how to use the ``NeighborSampler`` to\nsample subgraphs, and stores the nodes and edges of the subgraph, as\nwell as seed nodes in each iteration:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nx_G = nx.erdos_renyi_graph(36, 0.06)\nG = dgl.DGLGraph(nx_G.to_directed(), readonly=True)\nsampler = dgl.contrib.sampling.NeighborSampler(\n       G, 2, 3, num_hops=2, shuffle=True)\nnid = []\neid = []\nfor subg, aux_info in sampler:\n    nid.append(subg.parent_nid.asnumpy())\n    eid.append(subg.parent_eid.asnumpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sampler with DGL\n~~~~~~~~~~~~~~~~\n\nThe code illustrates the training process in mini-batches.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def update_embeddings_subgraph(g, seed_nodes, steady_state_operator):\n    # Note that we are only updating the embeddings of seed nodes here.\n    # The reason is that only the seed nodes have ample information\n    # from neighbors, especially if the subgraph is small (e.g. 1-hops)\n    prev_h = g.ndata['h'][seed_nodes]\n    next_h = steady_state_operator(g)[seed_nodes]\n    g.ndata['h'][seed_nodes] = (1 - alpha) * prev_h + alpha * next_h\n\ndef train_on_subgraphs(g, label_nodes, batch_size,\n                       steady_state_operator, predictor, trainer):\n    # To train SSE, we create two subgraph samplers with the\n    # `NeighborSampler` API for each phase.\n \n    # The first phase samples from all vertices in the graph.\n    sampler = dgl.contrib.sampling.NeighborSampler(\n            g, batch_size, g.number_of_nodes(), num_hops=1, return_seed_id=True)\n \n    # The second phase only samples from labeled vertices.\n    sampler_train = dgl.contrib.sampling.NeighborSampler(\n            g, batch_size, g.number_of_nodes(), seed_nodes=label_nodes, num_hops=1,\n            return_seed_id=True)\n    for i in range(n_embedding_updates):\n        subg, aux_info = next(sampler)\n        seeds = aux_info['seeds']\n        # Currently, subgraphing does not copy or share features\n        # automatically.  Therefore, we need to copy the node\n        # embeddings of the subgraph from the parent graph with\n        # `copy_from_parent()` before computing...\n        subg.copy_from_parent()\n        subg_seeds = subg.map_to_subgraph_nid(seeds)\n        update_embeddings_subgraph(subg, subg_seeds, steady_state_operator)\n        # ... and copy them back to the parent graph with\n        # `copy_to_parent()` afterwards.\n        subg.copy_to_parent()\n    for i in range(n_parameter_updates):\n        try:\n            subg, aux_info = next(sampler_train)\n            seeds = aux_info['seeds']\n        except:\n            break\n        # Again we need to copy features from parent graph\n        subg.copy_from_parent()\n        subg_seeds = subg.map_to_subgraph_nid(seeds)\n        loss = update_parameters(subg, subg_seeds,\n                                 steady_state_operator, predictor, trainer)\n        # We don't need to copy the features back to parent graph.\n    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also define a helper function that reports prediction accuracy:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def test(g, test_nodes, steady_state_operator, predictor):\n    predictor(g)\n    y_bar = mx.nd.argmax(g.ndata['z'], axis=1)[test_nodes]\n    y = g.ndata['y'].reshape(n)[test_nodes]\n    accuracy = mx.nd.sum(y_bar == y) / len(test_nodes)\n    return accuracy.asnumpy()[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Some routine preparations for training:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lr = 1e-3\nactivation = 'relu'\n\nsteady_state_operator = SteadyStateOperator(n_hidden, activation)\npredictor = Predictor(n_hidden, activation)\nsteady_state_operator.initialize()\npredictor.initialize()\nparams = steady_state_operator.collect_params()\nparams.update(predictor.collect_params())\ntrainer = gluon.Trainer(params, 'adam', {'learning_rate' : lr})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let\u2019s train it! As before, nodes reachable from $s$ are\ngradually \u201cinfected\u201d, except that behind the scene is a neural network!\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_epochs = 35\nn_embedding_updates = 8\nn_parameter_updates = 5\nalpha = 0.1\nbatch_size = 64\n\ny_bars = []\nfor i in range(n_epochs):\n    loss = train_on_subgraphs(g, nodes_train, batch_size, steady_state_operator, predictor, trainer)\n \n    accuracy_train = test(g, nodes_train, steady_state_operator, predictor)\n    accuracy_test = test(g, nodes_test, steady_state_operator, predictor)\n    print(\"Iter {:05d} | Train acc {:.4} | Test acc {:.4f}\".format(i, accuracy_train, accuracy_test))\n    y_bar = mx.nd.argmax(g.ndata['z'], axis=1)\n    y_bars.append(y_bar)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "|image2|\n\nIn this tutorial, we use a very small toy graph to demonstrate the\nsubgraph training for easy visualization. Subgraph training actually\nhelps us scale to gigantic graphs. For instance, we have successfully\nscaled SSE to a graph with 50 million nodes and 150 million edges in a\nsingle P3.8x large instance and one epoch only takes about 160 seconds.\n\nSee full examples `here <https://github.com/dmlc/dgl/tree/master/examples/mxnet/sse>`_.\n\n.. |image0| image:: https://s3.us-east-2.amazonaws.com/dgl.ai/tutorial/img/floodfill-paths.gif\n.. |image1| image:: https://s3.us-east-2.amazonaws.com/dgl.ai/tutorial/img/neighbor-sampling.gif\n.. |image2| image:: https://s3.us-east-2.amazonaws.com/dgl.ai/tutorial/img/sse.gif\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}